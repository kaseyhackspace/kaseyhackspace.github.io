<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-126992246-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  
  <title>The one about N-Grams and Markov Chains | kaseycodes</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Today we are going to make a Markov Chain to generate text from a training dataset we are going to feed it. So how do we start? We start by first implementing n-grams. N-grams are a list of n words th">
<meta name="keywords" content="python,numpy,n-grams,nlp,markov chains">
<meta property="og:type" content="article">
<meta property="og:title" content="The one about N-Grams and Markov Chains">
<meta property="og:url" content="https://kaseycodes.tk/2019/04/21/markov_chain/index.html">
<meta property="og:site_name" content="kaseycodes">
<meta property="og:description" content="Today we are going to make a Markov Chain to generate text from a training dataset we are going to feed it. So how do we start? We start by first implementing n-grams. N-grams are a list of n words th">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2019-04-21T07:06:46.639Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The one about N-Grams and Markov Chains">
<meta name="twitter:description" content="Today we are going to make a Markov Chain to generate text from a training dataset we are going to feed it. So how do we start? We start by first implementing n-grams. N-grams are a list of n words th">
  
    <link rel="alternate" href="/atom.xml" title="kaseycodes" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">kaseycodes</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/about-me">About Me</a>
        
          <a class="main-nav-link" href="/contact-me">Contact Me</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://kaseycodes.tk"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-markov_chain" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/04/21/markov_chain/" class="article-date">
  <time datetime="2019-04-21T00:30:00.000Z" itemprop="datePublished">2019-04-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      The one about N-Grams and Markov Chains
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Today we are going to make a Markov Chain to generate text from a training dataset we are going to feed it. So how do we start? We start by first implementing n-grams. N-grams are a list of n words that share a common border in a given string. It may sound confusing, so it is better that I implement it first then show by example:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NGram</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ngram</span><span class="params">(self, text, n=<span class="number">2</span>)</span>:</span></span><br><span class="line">        text_list = text.lower().split()</span><br><span class="line">        grams = [tuple(text_list[index:index+n]) <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">0</span>, len(text_list) - (n<span class="number">-1</span>))]</span><br><span class="line">        <span class="keyword">return</span> grams</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bigram</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.ngram(text, n=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">trigram</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.ngram(text, n=<span class="number">3</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>I defined a class called <code>NGram</code> that contains three functions. The first function <code>ngram</code> takes in a string variable <code>text</code> and an integer <code>n</code> to create an n-gram. By default, if no <code>n</code> is passed, then <code>n=2</code> and we get what&#x2019;s called a bigram (bi meaning 2). In line 4 I do some simple preprocessing of the text by converting everything to lowercase with the <code>lower</code> function and then splitting the string per word into a list with the <code>split</code> function (if you don&#x2019;t specify a separator in the <code>split</code> function, it splits the string by whitespace). n-grams are actually generated in line 5 where I generate a list of <code>tuples</code> using list indexing and list comprehension techniques.</p>
<p><code>bigram</code> and <code>trigram</code> functions are used to make n-grams where <code>n=2</code> and <code>n=3</code> respectively. As you can see, they use the <code>ngram</code> function under the hood. Lets have a few examples so you can see what the piece of code does on out sample text: &#x201C;Hello word, this is a sample text for consumption of the NGram class!&#x201D;</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sample = <span class="string">&quot;Hello word, this is a sample text for consumption of the NGram class!&quot;</span></span><br><span class="line">ngram = NGram()</span><br><span class="line"><span class="comment"># bi-gram example</span></span><br><span class="line">print(ngram.bigram(sample))</span><br></pre></td></tr></tbody></table></figure>
<pre><code>[(&apos;hello&apos;, &apos;word,&apos;), (&apos;word,&apos;, &apos;this&apos;), (&apos;this&apos;, &apos;is&apos;), (&apos;is&apos;, &apos;a&apos;), (&apos;a&apos;, &apos;sample&apos;), (&apos;sample&apos;, &apos;text&apos;), (&apos;text&apos;, &apos;for&apos;), (&apos;for&apos;, &apos;consumption&apos;), (&apos;consumption&apos;, &apos;of&apos;), (&apos;of&apos;, &apos;the&apos;), (&apos;the&apos;, &apos;ngram&apos;), (&apos;ngram&apos;, &apos;class!&apos;)]
</code></pre><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tri-gram example</span></span><br><span class="line">print(ngram.trigram(sample))</span><br></pre></td></tr></tbody></table></figure>
<pre><code>[(&apos;hello&apos;, &apos;word,&apos;, &apos;this&apos;), (&apos;word,&apos;, &apos;this&apos;, &apos;is&apos;), (&apos;this&apos;, &apos;is&apos;, &apos;a&apos;), (&apos;is&apos;, &apos;a&apos;, &apos;sample&apos;), (&apos;a&apos;, &apos;sample&apos;, &apos;text&apos;), (&apos;sample&apos;, &apos;text&apos;, &apos;for&apos;), (&apos;text&apos;, &apos;for&apos;, &apos;consumption&apos;), (&apos;for&apos;, &apos;consumption&apos;, &apos;of&apos;), (&apos;consumption&apos;, &apos;of&apos;, &apos;the&apos;), (&apos;of&apos;, &apos;the&apos;, &apos;ngram&apos;), (&apos;the&apos;, &apos;ngram&apos;, &apos;class!&apos;)]
</code></pre><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># n-gram example where n=4</span></span><br><span class="line">print(ngram.ngram(sample,n=<span class="number">4</span>))</span><br></pre></td></tr></tbody></table></figure>
<pre><code>[(&apos;hello&apos;, &apos;word,&apos;, &apos;this&apos;, &apos;is&apos;), (&apos;word,&apos;, &apos;this&apos;, &apos;is&apos;, &apos;a&apos;), (&apos;this&apos;, &apos;is&apos;, &apos;a&apos;, &apos;sample&apos;), (&apos;is&apos;, &apos;a&apos;, &apos;sample&apos;, &apos;text&apos;), (&apos;a&apos;, &apos;sample&apos;, &apos;text&apos;, &apos;for&apos;), (&apos;sample&apos;, &apos;text&apos;, &apos;for&apos;, &apos;consumption&apos;), (&apos;text&apos;, &apos;for&apos;, &apos;consumption&apos;, &apos;of&apos;), (&apos;for&apos;, &apos;consumption&apos;, &apos;of&apos;, &apos;the&apos;), (&apos;consumption&apos;, &apos;of&apos;, &apos;the&apos;, &apos;ngram&apos;), (&apos;of&apos;, &apos;the&apos;, &apos;ngram&apos;, &apos;class!&apos;)]
</code></pre><p>I do hope the examples above made n-grams clearer than I could explain them :). Now that we&#x2019;ve implemented our NGram class, we can start building our shiny Markov Chain! The markov chain class is a bit lengthy, so I chose to write docstrings under the function names instead so you have have a high level view of the class and dig in to investigate the code if you want.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MarkovChain</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Initialization creates empty dictionary</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.model = {}</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate_probabilities</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        per word in model, calculate the probability of next words following the current word</span></span><br><span class="line"><span class="string">        based on the bigram pair and how many times the bigram pair appears in the training dataset</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> self.model:</span><br><span class="line">            total_sum = np.sum(self.model[key][<span class="string">&apos;counts&apos;</span>])</span><br><span class="line">            self.model[key][<span class="string">&apos;probability&apos;</span>] = [x/total_sum <span class="keyword">for</span> x <span class="keyword">in</span> self.model[key][<span class="string">&apos;counts&apos;</span>]]</span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(self,text)</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        The train_model method first generates bigrams from the given text string. The method then</span></span><br><span class="line"><span class="string">        traverses each bigram and determines if the first word in the bigram already exists in the model.</span></span><br><span class="line"><span class="string">        The behavior of the method per bigram is determined with the following cases:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        If the first element has not been added yet(does not have a key in the dictionary), the first </span></span><br><span class="line"><span class="string">        element is added to the dictionary with the second element added in the `values` list with a </span></span><br><span class="line"><span class="string">        corresponding occurance count of 1.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        If the first element has already been added but the second element has not yet been added to the</span></span><br><span class="line"><span class="string">        `values` list, it appends the second element to the `values` list with a corresponding occurance count </span></span><br><span class="line"><span class="string">        of 1. </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        If both the first and second elements of the bigram already exist, just increment the number of</span></span><br><span class="line"><span class="string">        occurances of the second element by 1.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        After all bigrams have been processed, the train_model method calls the calculate_probability method</span></span><br><span class="line"><span class="string">        to calculate the probability of choosing the next word randomly given the current work in the Markov</span></span><br><span class="line"><span class="string">        Chain.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        The final model would have the following structure:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        {</span></span><br><span class="line"><span class="string">            &apos;word-1&apos;: {</span></span><br><span class="line"><span class="string">                values: [&lt;list_of_probable_words&gt;]</span></span><br><span class="line"><span class="string">                counts: [&lt;number_of_occurences_per_word_in_values&gt;]</span></span><br><span class="line"><span class="string">                probability: [&lt;probability_of_occurence_per_word_in_values&gt;]</span></span><br><span class="line"><span class="string">            },</span></span><br><span class="line"><span class="string">            &apos;word-2&apos;: {</span></span><br><span class="line"><span class="string">                values: [&lt;list_of_probable_words&gt;]</span></span><br><span class="line"><span class="string">                counts: [&lt;number_of_occurences_per_word_in_values&gt;]</span></span><br><span class="line"><span class="string">                probability: [&lt;probability_of_occurence_per_word_in_values&gt;]</span></span><br><span class="line"><span class="string">            },</span></span><br><span class="line"><span class="string">            ...,</span></span><br><span class="line"><span class="string">            &apos;word-n&apos;: {</span></span><br><span class="line"><span class="string">                values: [&lt;list_of_probable_words&gt;]</span></span><br><span class="line"><span class="string">                counts: [&lt;number_of_occurences_per_word_in_values&gt;]</span></span><br><span class="line"><span class="string">                probability: [&lt;probability_of_occurence_per_word_in_values&gt;]</span></span><br><span class="line"><span class="string">            }</span></span><br><span class="line"><span class="string">        }</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        ngram = NGram()</span><br><span class="line">        </span><br><span class="line">        bigrams = ngram.bigram(text)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> bigram <span class="keyword">in</span> bigrams:</span><br><span class="line">            <span class="keyword">if</span> bigram[<span class="number">0</span>] <span class="keyword">not</span> <span class="keyword">in</span> self.model:</span><br><span class="line">                self.model[bigram[<span class="number">0</span>]]= {</span><br><span class="line">                    <span class="string">&apos;values&apos;</span>: [bigram[<span class="number">1</span>]],</span><br><span class="line">                    <span class="string">&apos;counts&apos;</span>: [<span class="number">1</span>],</span><br><span class="line">                    <span class="string">&apos;probability&apos;</span>: []</span><br><span class="line">                }</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> bigram[<span class="number">1</span>] <span class="keyword">not</span> <span class="keyword">in</span> self.model[bigram[<span class="number">0</span>]][<span class="string">&apos;values&apos;</span>]:</span><br><span class="line">                    self.model[bigram[<span class="number">0</span>]][<span class="string">&apos;values&apos;</span>].append(bigram[<span class="number">1</span>])</span><br><span class="line">                    self.model[bigram[<span class="number">0</span>]][<span class="string">&apos;counts&apos;</span>].append(<span class="number">1</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    index = self.model[bigram[<span class="number">0</span>]][<span class="string">&apos;values&apos;</span>].index(bigram[<span class="number">1</span>])</span><br><span class="line">                    self.model[bigram[<span class="number">0</span>]][<span class="string">&apos;counts&apos;</span>][index] = self.model[bigram[<span class="number">0</span>]][<span class="string">&apos;counts&apos;</span>][index] + <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            self.calculate_probabilities()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_text</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        This method generates a random sentence of n words. It selects a random starting point from the model</span></span><br><span class="line"><span class="string">        then chooses a random hop to the next word based on the probability array of the word list n times.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Note that it is possible to have a word that does not have next hops, we gracefully handle that with</span></span><br><span class="line"><span class="string">        a try except case that returns the sentence prematurely if ever that happens.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        sentence = <span class="string">&apos;&apos;</span></span><br><span class="line">        current_word = np.random.choice(list(self.model.keys()))</span><br><span class="line">        sentence += current_word</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                current_word = np.random.choice(self.model[current_word][<span class="string">&apos;values&apos;</span>],p=self.model[current_word][<span class="string">&apos;probability&apos;</span>])</span><br><span class="line">                sentence += <span class="string">&apos; &apos;</span>+current_word</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                <span class="keyword">return</span> sentence   </span><br><span class="line">        <span class="keyword">return</span> sentence</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_sentence</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        This method generates a random sentence of . It selects a random starting point from the model</span></span><br><span class="line"><span class="string">        then chooses a random hop to the next word based on the probability array of the word list until</span></span><br><span class="line"><span class="string">        it encounters a &quot;.&quot;.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Note that it is possible to have a word that does not have next hops, we gracefully handle that with</span></span><br><span class="line"><span class="string">        a try except case that returns the sentence prematurely if ever that happens.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        sentence = <span class="string">&apos;&apos;</span></span><br><span class="line">        current_word = np.random.choice(list(self.model.keys()))</span><br><span class="line">        sentence += current_word</span><br><span class="line">        <span class="keyword">while</span> <span class="string">&apos;.&apos;</span> <span class="keyword">not</span> <span class="keyword">in</span> current_word:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                current_word = np.random.choice(self.model[current_word][<span class="string">&apos;values&apos;</span>],p=self.model[current_word][<span class="string">&apos;probability&apos;</span>])</span><br><span class="line">                sentence += <span class="string">&apos; &apos;</span>+current_word</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                <span class="keyword">return</span> sentence   </span><br><span class="line">        <span class="keyword">return</span> sentence</span><br></pre></td></tr></tbody></table></figure>
<p>Now that we have the Markov Chain class implemented, we can test out the code! For the training dataset, I chose wikipedia&#x2019;s summary of the <a href="https://en.wikipedia.org/wiki/Avengers:_Infinity_War#Plot" target="_blank" rel="noopener">Avengers: infinity war</a> movie.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">training_text = <span class="string">&quot;Having acquired the Power Stone, one of the six Infinity Stones, from the planet Xandar, Thanos and his lieutenants&#x2014;Ebony Maw, Cull Obsidian, Proxima Midnight, and Corvus Glaive&#x2014;intercept the spaceship carrying the surviving Asgardians. As they extract the Space Stone from the Tesseract, Thanos subdues Thor, overpowers Hulk, and kills Loki. Heimdall sends Hulk to Earth using the Bifr&#xF6;st before being killed. Thanos departs with his lieutenants and obliterates the ship. Hulk crash-lands at the Sanctum Sanctorum in New York City, reverting to Bruce Banner. He warns Stephen Strange and Wong about Thanos&apos; plan to kill half of all life in the universe; in response, Strange recruits Tony Stark. Maw and Obsidian arrive to retrieve the Time Stone from Strange, drawing the attention of Peter Parker. Maw captures Strange, but fails to take the Time Stone due to an enchantment. Stark and Parker pursue Maw&apos;s spaceship, Banner contacts Steve Rogers, and Wong stays behind to guard the Sanctum. In Edinburgh, Midnight and Glaive ambush Wanda Maximoff and Vision in order to retrieve the Mind Stone in Vision&apos;s forehead. Rogers, Natasha Romanoff, and Sam Wilson rescue them and take shelter with James Rhodes and Banner at the Avengers Facility. Vision offers to sacrifice himself by having Maximoff destroy the Mind Stone to keep Thanos from retrieving it. Rogers suggests they travel to Wakanda, which he believes has the resources to remove the stone without destroying Vision. The Guardians of the Galaxy respond to a distress call from the Asgardian ship and rescue Thor, who surmises that Thanos seeks the Reality Stone, which is in the possession of the Collector on Knowhere. Rocket and Groot accompany Thor to Nidavellir, where they and Eitri create Stormbreaker, a battle-axe capable of killing Thanos. On Knowhere, Peter Quill, Gamora, Drax, and Mantis find Thanos with the Reality Stone already in his possession. Thanos kidnaps Gamora, his adopted daughter, who reveals the location of the Soul Stone to save her captive adopted sister Nebula from torture. Thanos and Gamora travel to Vormir, where Red Skull, keeper of the Soul Stone, informs him the stone can only be retrieved by sacrificing someone he loves. Thanos reluctantly kills Gamora, earning the stone. Nebula escapes captivity and asks the remaining Guardians to meet her on Thanos&apos; destroyed homeworld, Titan. Stark and Parker kill Maw and rescue Strange. Landing on Titan, they meet Quill, Drax, and Mantis. The group forms a plan to remove Thanos&apos; Infinity Gauntlet after Strange uses the Time Stone to view millions of possible futures, seeing only one in which Thanos loses. Thanos arrives, justifying his plans as necessary to ensure the survival of a universe threatened by overpopulation. The group subdues him until Nebula deduces that Thanos has killed Gamora. Enraged, Quill attacks him, allowing Thanos to break the group&apos;s hold and overpower them. Stark is seriously wounded by Thanos, but is spared after Strange surrenders the Time Stone to Thanos. In Wakanda, Rogers reunites with Bucky Barnes before Thanos&apos; army invades. The Avengers, alongside T&apos;Challa and the Wakandan forces, mount a defense while Shuri works to extract the Mind Stone from Vision. Banner, unable to transform into the Hulk, fights in Stark&apos;s Hulkbuster armor. Thor, Rocket, and Groot arrive to reinforce the Avengers; Midnight, Obsidian, and Glaive are killed and their army is routed. Thanos arrives and despite Maximoff&apos;s attempt to destroy the Mind Stone, removes it from Vision&apos;s head, killing him.Thor severely wounds Thanos, but Thanos activates the completed Infinity Gauntlet and teleports away. Half of all life across the universe disintegrates, including Barnes, T&apos;Challa, Groot, Maximoff, Wilson, Mantis, Drax, Quill, Strange, and Parker, as well as Maria Hill and Nick Fury, although Fury is able to transmit a signal to Carol Danvers first. Stark and Nebula remain on Titan while Banner, M&apos;Baku, Okoye, Rhodes, Rocket, Rogers, Romanoff, and Thor are left on the Wakandan battlefield. Meanwhile, Thanos watches a sunrise on another planet.&quot;</span></span><br></pre></td></tr></tbody></table></figure>
<p>In the code below I try splitting the model up first by sentence then run a <code>for</code> loop to train the model per sentence.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">chain = MarkovChain()</span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> training_text.split(<span class="string">&apos;.&apos;</span>):</span><br><span class="line">    chain.train_model(sentence)</span><br></pre></td></tr></tbody></table></figure>
<p>Let&#x2019;s see what the model comes up with when we generate 10 sentences that are 20 words long:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">11</span>):</span><br><span class="line">    print(str(index)+<span class="string">&apos;. &apos;</span>+chain.generate_text(<span class="number">20</span>))</span><br></pre></td></tr></tbody></table></figure>
<pre><code>1. wanda maximoff and sam wilson rescue strange recruits tony stark is able to thanos to earth using the sanctum sanctorum in
2. midnight, and wong stays behind to earth using the soul stone, one of the universe disintegrates, including barnes, t&apos;challa, groot, maximoff,
3. rescue them and despite maximoff&apos;s attempt to view millions of the tesseract, thanos departs with james rhodes and mantis find thanos
4. knowhere, peter quill, gamora, his lieutenants&#x2014;ebony maw, cull obsidian, proxima midnight, obsidian, and parker, as they travel to reinforce the remaining
5. which is seriously wounded by overpopulation
6. distress call from vision&apos;s forehead
7. group&apos;s hold and teleports away
8. fury is seriously wounded by thanos, but fails to an enchantment
9. acquired the possession of the collector on thanos&apos; destroyed homeworld, titan while banner, m&apos;baku, okoye, rhodes, rocket, rogers, and rescue strange
10. remove the time stone without destroying vision in new york city, reverting to thanos and groot arrive to guard the ship
</code></pre><p>Now let&#x2019;s try training the model by feeding it the whole <code>training_text</code> corpus instead and generate sentences that end in a period (.):</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">chain = MarkovChain()</span><br><span class="line">chain.train_model(training_text)    </span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">11</span>):</span><br><span class="line">    print(str(index)+<span class="string">&apos;. &apos;</span>+chain.generate_sentence())</span><br></pre></td></tr></tbody></table></figure>
<pre><code>1. universe; in stark&apos;s hulkbuster armor.
2. be retrieved by sacrificing someone he believes has killed and despite maximoff&apos;s attempt to remove thanos&apos; plan to transform into the time stone to earth using the group forms a distress call from strange, drawing the wakandan battlefield.
3. midnight, and parker kill half of killing thanos.
4. all life in vision&apos;s forehead.
5. survival of killing him.thor
6. but thanos seeks the asgardian ship and nick fury, although fury is seriously wounded by overpopulation.
7. thor to save her on titan, they and asks the sanctum.
8. distress call from strange, drawing the planet xandar, thanos arrives and gamora travel to earth using the possession of peter parker.
9. wilson rescue strange.
10. knowhere.
</code></pre><p>Hopefully you found the sentences generated by the Markov Chain amusing :D. For more information about n-grams and Markov Chain, just do a regular Google search and that would lead you to the right direction!</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://kaseycodes.tk/2019/04/21/markov_chain/" data-id="cjxiy14z0001pjofocy04nora" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/markov-chains/">markov chains</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/n-grams/">n-grams</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/nlp/">nlp</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/numpy/">numpy</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/06/24/Interpreting-Marcus-Aurelius-1/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Interpreting Marcus Aurelius 1
        
      </div>
    </a>
  
  
    <a href="/2018/12/03/Project-Compozen-Build-Log-0/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Project Compozen: Build Log 0</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/18-04/">18.04</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/c/">c</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/c/">c++</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cli/">cli</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/compiler/">compiler</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/directing-mind/">directing mind</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/docker/">docker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/docker-compose/">docker-compose</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/drivers/">drivers</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/gcc/">gcc</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/grub/">grub</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/marcus-aurelius/">marcus aurelius</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/markov-chains/">markov chains</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/n-grams/">n-grams</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nature/">nature</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nlp/">nlp</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/node-js/">node.js</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/numpy/">numpy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nvidia/">nvidia</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/opensource/">opensource</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/procastination/">procastination</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/stoic/">stoic</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/stoicism/">stoicism</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/time/">time</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ubuntu/">ubuntu</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/18-04/" style="font-size: 10px;">18.04</a> <a href="/tags/c/" style="font-size: 10px;">c</a> <a href="/tags/c/" style="font-size: 10px;">c++</a> <a href="/tags/cli/" style="font-size: 10px;">cli</a> <a href="/tags/compiler/" style="font-size: 10px;">compiler</a> <a href="/tags/directing-mind/" style="font-size: 10px;">directing mind</a> <a href="/tags/docker/" style="font-size: 10px;">docker</a> <a href="/tags/docker-compose/" style="font-size: 10px;">docker-compose</a> <a href="/tags/drivers/" style="font-size: 10px;">drivers</a> <a href="/tags/gcc/" style="font-size: 10px;">gcc</a> <a href="/tags/grub/" style="font-size: 10px;">grub</a> <a href="/tags/marcus-aurelius/" style="font-size: 20px;">marcus aurelius</a> <a href="/tags/markov-chains/" style="font-size: 10px;">markov chains</a> <a href="/tags/n-grams/" style="font-size: 10px;">n-grams</a> <a href="/tags/nature/" style="font-size: 10px;">nature</a> <a href="/tags/nlp/" style="font-size: 10px;">nlp</a> <a href="/tags/node-js/" style="font-size: 10px;">node.js</a> <a href="/tags/numpy/" style="font-size: 10px;">numpy</a> <a href="/tags/nvidia/" style="font-size: 10px;">nvidia</a> <a href="/tags/opensource/" style="font-size: 10px;">opensource</a> <a href="/tags/procastination/" style="font-size: 10px;">procastination</a> <a href="/tags/python/" style="font-size: 10px;">python</a> <a href="/tags/stoic/" style="font-size: 20px;">stoic</a> <a href="/tags/stoicism/" style="font-size: 20px;">stoicism</a> <a href="/tags/time/" style="font-size: 10px;">time</a> <a href="/tags/ubuntu/" style="font-size: 10px;">ubuntu</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">April 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/06/30/Interpreting-Marcus-Aurelius-4/">Interpreting Marcus Aurelius 4</a>
          </li>
        
          <li>
            <a href="/2019/06/28/Interpreting-Marcus-Aurelius-3/">Interpreting Marcus Aurelius 3</a>
          </li>
        
          <li>
            <a href="/2019/06/25/Interpreting-Marcus-Aurelius-2/">Interpreting Marcus Aurelius 2</a>
          </li>
        
          <li>
            <a href="/2019/06/24/Interpreting-Marcus-Aurelius-1/">Interpreting Marcus Aurelius 1</a>
          </li>
        
          <li>
            <a href="/2019/04/21/markov_chain/">The one about N-Grams and Markov Chains</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Kasey Martin<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/about-me" class="mobile-nav-link">About Me</a>
  
    <a href="/contact-me" class="mobile-nav-link">Contact Me</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>